# Supervisor meet January 14th
Feedback:
- As a general question yes, 
- Originality! Maybe find a more specific one, that’s a subquestion of our current RQ, especially if we’re doing a comparison 
  - How are we going to evaluate our data? The RQ is very open 
  - Would also make it easier for us to interpret our results 
  - E.g. using carissa chew’s vocabulary (e.g. “how do they frame it according to the vocabulary? How many of the topics in the vocabulary are covered? How many instances in the articles are problematic?
  - Either phrase the specific aspect in the RQ already or make it clear in the work 
- Dataset collection: if it’s hard, find another source (incl. Datasets someone else made)
  - Anything works, as long as we make sure to specify our selection criteria!
  - Try kaggle or other dataset repositories 
  - Google scholar: use research papers 
- Start on even just one article already 
- Analysis unit: try it out and base the decision on the results 
- Can send Maddalena criteria and keywords/the dataset 
- Can also split work: two people manually collect dataset, two people try web-scraping

Next steps:  
- Make the RQ more specific (think of methods and what and how to evaluate)
- Decide on/construct dataset
- Try out our methods on at least one article 
- Next meeting: Friday at 10 
